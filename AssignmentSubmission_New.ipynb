{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da936247",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import statements\n",
    "import os\n",
    "import subprocess\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType\n",
    "from pyspark.sql.functions import size\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import *\n",
    "import glob\n",
    "import pyspark.sql.functions as func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "088315e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/02 16:33:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/04/02 16:33:11 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/04/02 16:33:11 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/04/02 16:33:11 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "23/04/02 16:33:11 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "23/04/02 16:33:11 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://himakars-mbp.lan:4045\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fa5f0ae19a0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1361e0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_local = \"/Users/himakarananthasetty/Downloads/SEM2/CloudComp/Assignment5/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "779b3e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "year=2010\n",
    "set_the_df = {}\n",
    "while year<=2022: #As the data is till 2022\n",
    "    data = glob.glob(data_local + \"/\"+str(year)+\"/*.csv\")\n",
    "    set_the_df[\"csv_df_{0}\".format(year)]  = spark.read.option(\"header\", \"true\").option(\"delimiter\",\",\").csv(data,inferSchema=True)\n",
    "    year+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07d9a4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find the hottest day (column MAX) for each year, and provide the corresponding station code, station name and the date (columns STATION, NAME, DATE).   > There should be 13 results\n",
      "Note: The value of 9999.9 will be disregarded as it is treated as a null or missing value.\n",
      "The 13 results are below with column MAX\n",
      "23/04/02 16:33:15 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "1\n",
      "2010\n",
      "+-----------+--------------------+-------------------+----+\n",
      "|    STATION|                NAME|               DATE| MAX|\n",
      "+-----------+--------------------+-------------------+----+\n",
      "|99407099999|DESTRUCTION IS. W...|2010-08-15 00:00:00|74.8|\n",
      "+-----------+--------------------+-------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Question1\n",
    "print(\"Find the hottest day (column MAX) for each year, and provide the corresponding station code, station name and the date (columns STATION, NAME, DATE).   > There should be 13 results\")\n",
    "print(\"Note: The value of 9999.9 will be disregarded as it is treated as a null or missing value.\")\n",
    "print(\"The 13 results are below with column MAX\")\n",
    "year=2010\n",
    "i=1\n",
    "set_the_df[\"csv_df_\"+str(year)].createOrReplaceTempView(\"temporary_elements\")\n",
    "print(i)\n",
    "print(year)\n",
    "result = spark.sql(\"SELECT STATION, NAME, DATE, MAX FROM temporary_elements where MAX =(select MAX((MAX)) from temporary_elements where MAX!=9999.9) limit 1 \").show()\n",
    "year=year+1\n",
    "i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "521fbd24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2011\n",
      "+----------+-------------+-------------------+----+\n",
      "|   STATION|         NAME|               DATE| MAX|\n",
      "+----------+-------------+-------------------+----+\n",
      "|1046099999|SORKJOSEN, NO|2011-07-09 00:00:00|87.8|\n",
      "+----------+-------------+-------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Question1\n",
    "set_the_df[\"csv_df_\"+str(year)].createOrReplaceTempView(\"temporary_elements\")\n",
    "print(i)\n",
    "print(year)\n",
    "\n",
    "result = spark.sql(\"SELECT STATION, NAME, DATE, MAX FROM temporary_elements where MAX =(select MAX((MAX)) from temporary_elements where MAX!=9999.9) limit 1 \").show()\n",
    "i+=1\n",
    "year+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33ad07bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "2012\n",
      "+----------+-------------+-------------------+----+\n",
      "|   STATION|         NAME|               DATE| MAX|\n",
      "+----------+-------------+-------------------+----+\n",
      "|1023099999|BARDUFOSS, NO|2012-07-05 00:00:00|72.0|\n",
      "+----------+-------------+-------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Question1\n",
    "set_the_df[\"csv_df_\"+str(year)].createOrReplaceTempView(\"temporary_elements\")\n",
    "print(i)\n",
    "print(year)\n",
    "\n",
    "result = spark.sql(\"SELECT STATION, NAME, DATE, MAX FROM temporary_elements where MAX =(select MAX((MAX)) from temporary_elements where MAX!=9999.9) limit 1 \").show()\n",
    "i+=1\n",
    "year+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "266c59db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "2013\n",
      "+----------+--------------+-------------------+----+\n",
      "|   STATION|          NAME|               DATE| MAX|\n",
      "+----------+--------------+-------------------+----+\n",
      "|1001499999|SORSTOKKEN, NO|2013-08-02 00:00:00|80.6|\n",
      "+----------+--------------+-------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Question1\n",
    "set_the_df[\"csv_df_\"+str(year)].createOrReplaceTempView(\"temporary_elements\")\n",
    "print(i)\n",
    "print(year)\n",
    "\n",
    "result = spark.sql(\"SELECT STATION, NAME, DATE, MAX FROM temporary_elements where MAX =(select MAX((MAX)) from temporary_elements where MAX!=9999.9) limit 1 \").show()\n",
    "i+=1\n",
    "year+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e298e0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "2014\n",
      "+----------+-------------+-------------------+----+\n",
      "|   STATION|         NAME|               DATE| MAX|\n",
      "+----------+-------------+-------------------+----+\n",
      "|1023099999|BARDUFOSS, NO|2014-07-10 00:00:00|89.6|\n",
      "+----------+-------------+-------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Question1\n",
    "set_the_df[\"csv_df_\"+str(year)].createOrReplaceTempView(\"temporary_elements\")\n",
    "print(i)\n",
    "print(year)\n",
    "\n",
    "result = spark.sql(\"SELECT STATION, NAME, DATE, MAX FROM temporary_elements where MAX =(select MAX((MAX)) from temporary_elements where MAX!=9999.9) limit 1 \").show()\n",
    "i+=1\n",
    "year+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e59c1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "2015\n",
      "+----------+----------+-------------------+----+\n",
      "|   STATION|      NAME|               DATE| MAX|\n",
      "+----------+----------+-------------------+----+\n",
      "|1025099999|TROMSO, NO|2015-07-30 00:00:00|71.6|\n",
      "+----------+----------+-------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Question1\n",
    "set_the_df[\"csv_df_\"+str(year)].createOrReplaceTempView(\"temporary_elements\")\n",
    "print(i)\n",
    "print(year)\n",
    "\n",
    "result = spark.sql(\"SELECT STATION, NAME, DATE, MAX FROM temporary_elements where MAX =(select MAX((MAX)) from temporary_elements where MAX!=9999.9) limit 1 \").show()\n",
    "i+=1\n",
    "year+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b2a0f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "2016\n",
      "+----------+-----------+-------------------+----+\n",
      "|   STATION|       NAME|               DATE| MAX|\n",
      "+----------+-----------+-------------------+----+\n",
      "|1023199999|DRAUGEN, NO|2016-07-21 00:00:00|77.0|\n",
      "+----------+-----------+-------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Question1\n",
    "set_the_df[\"csv_df_\"+str(year)].createOrReplaceTempView(\"temporary_elements\")\n",
    "print(i)\n",
    "print(year)\n",
    "\n",
    "result = spark.sql(\"SELECT STATION, NAME, DATE, MAX FROM temporary_elements where MAX =(select MAX((MAX)) from temporary_elements where MAX!=9999.9) limit 1 \").show()\n",
    "i+=1\n",
    "year+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cc1869b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "2017\n",
      "+----------+-------------+-------------------+----+\n",
      "|   STATION|         NAME|               DATE| MAX|\n",
      "+----------+-------------+-------------------+----+\n",
      "|1023099999|BARDUFOSS, NO|2017-06-09 00:00:00|78.6|\n",
      "+----------+-------------+-------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Question1\n",
    "set_the_df[\"csv_df_\"+str(year)].createOrReplaceTempView(\"temporary_elements\")\n",
    "print(i)\n",
    "print(year)\n",
    "\n",
    "result = spark.sql(\"SELECT STATION, NAME, DATE, MAX FROM temporary_elements where MAX =(select MAX((MAX)) from temporary_elements where MAX!=9999.9) limit 1 \").show()\n",
    "i+=1\n",
    "year+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e3d2fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "2018\n",
      "+----------+----------+-------------------+----+\n",
      "|   STATION|      NAME|               DATE| MAX|\n",
      "+----------+----------+-------------------+----+\n",
      "|1025099999|TROMSO, NO|2018-07-29 00:00:00|84.2|\n",
      "+----------+----------+-------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Question1\n",
    "set_the_df[\"csv_df_\"+str(year)].createOrReplaceTempView(\"temporary_elements\")\n",
    "print(i)\n",
    "print(year)\n",
    "\n",
    "result = spark.sql(\"SELECT STATION, NAME, DATE, MAX FROM temporary_elements where MAX =(select MAX((MAX)) from temporary_elements where MAX!=9999.9) limit 1 \").show()\n",
    "i+=1\n",
    "year+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "940bd462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "2019\n",
      "+----------+-------------+-------------------+----+\n",
      "|   STATION|         NAME|               DATE| MAX|\n",
      "+----------+-------------+-------------------+----+\n",
      "|1023099999|BARDUFOSS, NO|2019-07-21 00:00:00|78.8|\n",
      "+----------+-------------+-------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Question1\n",
    "set_the_df[\"csv_df_\"+str(year)].createOrReplaceTempView(\"temporary_elements\")\n",
    "print(i)\n",
    "print(year)\n",
    "\n",
    "result = spark.sql(\"SELECT STATION, NAME, DATE, MAX FROM temporary_elements where MAX =(select MAX((MAX)) from temporary_elements where MAX!=9999.9) limit 1 \").show()\n",
    "i+=1\n",
    "year+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2660293b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "2020\n",
      "+----------+-------------+-------------------+----+\n",
      "|   STATION|         NAME|               DATE| MAX|\n",
      "+----------+-------------+-------------------+----+\n",
      "|1023099999|BARDUFOSS, NO|2020-06-22 00:00:00|79.9|\n",
      "+----------+-------------+-------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Question1\n",
    "set_the_df[\"csv_df_\"+str(year)].createOrReplaceTempView(\"temporary_elements\")\n",
    "print(i)\n",
    "print(year)\n",
    "\n",
    "result = spark.sql(\"SELECT STATION, NAME, DATE, MAX FROM temporary_elements where MAX =(select MAX((MAX)) from temporary_elements where MAX!=9999.9) limit 1 \").show()\n",
    "i+=1\n",
    "year+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "325dc5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "2021\n",
      "+----------+------------+-------------------+----+\n",
      "|   STATION|        NAME|               DATE| MAX|\n",
      "+----------+------------+-------------------+----+\n",
      "|1065099999|KARASJOK, NO|2021-07-05 00:00:00|88.3|\n",
      "+----------+------------+-------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Question1\n",
    "set_the_df[\"csv_df_\"+str(year)].createOrReplaceTempView(\"temporary_elements\")\n",
    "print(i)\n",
    "print(year)\n",
    "\n",
    "result = spark.sql(\"SELECT STATION, NAME, DATE, MAX FROM temporary_elements where MAX =(select MAX((MAX)) from temporary_elements where MAX!=9999.9) limit 1 \").show()\n",
    "i+=1\n",
    "year+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf4002f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "2022\n",
      "+----------+----------+-------------------+----+\n",
      "|   STATION|      NAME|               DATE| MAX|\n",
      "+----------+----------+-------------------+----+\n",
      "|2095099999|PAJALA, SW|2022-07-01 00:00:00|85.5|\n",
      "+----------+----------+-------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Question1\n",
    "set_the_df[\"csv_df_\"+str(year)].createOrReplaceTempView(\"temporary_elements\")\n",
    "print(i)\n",
    "print(year)\n",
    "\n",
    "result = spark.sql(\"SELECT STATION, NAME, DATE, MAX FROM temporary_elements where MAX =(select MAX((MAX)) from temporary_elements where MAX!=9999.9) limit 1 \").show()\n",
    "i+=1\n",
    "year+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "485373ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Find the coldest day (column MIN) for the month of January across all years (2010 - 2022) , and provide the corresponding station code, station name and the date (columns STATION, NAME, DATE). > There should be 1 result\n",
      "Note: The value of 9999.9 will be disregarded as it is treated as a null or missing value.\n",
      "The 1 result is below with column MIN\n",
      "+----------+-------------+-------------------+-----+\n",
      "|   STATION|         NAME|               DATE|  MIN|\n",
      "+----------+-------------+-------------------+-----+\n",
      "|1023099999|BARDUFOSS, NO|2017-01-05 00:00:00|-28.3|\n",
      "+----------+-------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Question2\n",
    "print(\"2. Find the coldest day (column MIN) for the month of January across all years (2010 - 2022) , and provide the corresponding station code, station name and the date (columns STATION, NAME, DATE). > There should be 1 result\")\n",
    "print(\"Note: The value of 9999.9 will be disregarded as it is treated as a null or missing value.\")\n",
    "print(\"The 1 result is below with column MIN\")\n",
    "dfs = [set_the_df[\"csv_df_2010\"],set_the_df[\"csv_df_2011\"],set_the_df[\"csv_df_2012\"],set_the_df[\"csv_df_2013\"],set_the_df[\"csv_df_2014\"],set_the_df[\"csv_df_2015\"],set_the_df[\"csv_df_2016\"],set_the_df[\"csv_df_2017\"],set_the_df[\"csv_df_2018\"],set_the_df[\"csv_df_2019\"],set_the_df[\"csv_df_2020\"],set_the_df[\"csv_df_2021\"],set_the_df[\"csv_df_2022\"]]\n",
    "dataframe = reduce(DataFrame.unionAll, dfs)\n",
    "\n",
    "dataframe.createOrReplaceTempView(\"temporary_elements\")\n",
    "res = spark.sql(\"SELECT STATION, NAME, DATE, MIN FROM temporary_elements where MIN=(select MIN((MIN)) from temporary_elements where MIN!=9999.9 and  month(DATE)=1) limit 1 \").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "722ebb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Maximum and Minimum precipitation (column PRCP ) for the year 2015, and provide the corresponding station code, station name and the date (columns STATION, NAME, DATE). > There should be 2 results.  Any max or min would do.  Just choose 1 or each.\n",
      "NOTE: 0 and 99.9 is ignored as - “0” indicates no measure of precipitation as per the code (includes a trace).Missing = 99.99)\n",
      "Chose both, so the 2 results are below with column PRCP of year 2015\n",
      "+----------+------------+-------------------+----+\n",
      "|   STATION|        NAME|               DATE|PRCP|\n",
      "+----------+------------+-------------------+----+\n",
      "|1008099999|LONGYEAR, SV|2015-01-18 00:00:00|0.01|\n",
      "|1025099999|  TROMSO, NO|2015-11-02 00:00:00|2.11|\n",
      "+----------+------------+-------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Question3\n",
    "print(\"3. Maximum and Minimum precipitation (column PRCP ) for the year 2015, and provide the corresponding station code, station name and the date (columns STATION, NAME, DATE). > There should be 2 results.  Any max or min would do.  Just choose 1 or each.\")\n",
    "print(\"NOTE: 0 and 99.9 is ignored as - “0” indicates no measure of precipitation as per the code (includes a trace).Missing = 99.99)\")\n",
    "print(\"Chose both, so the 2 results are below with column PRCP of year 2015\")\n",
    "\n",
    "set_the_df[\"csv_df_2015\"].createOrReplaceTempView(\"temporary_elements\")\n",
    "\n",
    "res = spark.sql(\"(SELECT STATION, NAME, DATE, PRCP FROM temporary_elements where PRCP=(select MIN((PRCP)) from temporary_elements where PRCP!=0.00) limit 1) UNION (SELECT STATION, NAME, DATE, PRCP FROM temporary_elements where PRCP=(select MAX((PRCP)) from temporary_elements where PRCP!=99.99) limit 1) \").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0e70194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.Count percentage missing values for wind gust (column GUST) for the year 2019. > There should be 1 result\n",
      "Note: The value of 999.99 will be disregarded as it is treated as a null or missing value.\n",
      "The 1 result is below\n",
      "+-----------------+\n",
      "|    Missing_Value|\n",
      "+-----------------+\n",
      "|82.87671232876713|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Question4\n",
    "print(\"4.Count percentage missing values for wind gust (column GUST) for the year 2019. > There should be 1 result\")\n",
    "print(\"Note: The value of 999.99 will be disregarded as it is treated as a null or missing value.\")\n",
    "print(\"The 1 result is below\")\n",
    "\n",
    "set_the_df[\"csv_df_2019\"].createOrReplaceTempView(\"temporary_elements\")\n",
    "res= spark.sql(\"select (Select COUNT(GUST) from temporary_elements where GUST=999.9) * 100/count(GUST) as Missing_Value from temporary_elements\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4fcdcf00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.Find the mean, median, mode and standard deviation of the Temperature (column TEMP) for each month for the year 2020. > There should be 12 results for each month\n",
      "The four results Mean, Standard_Deviation, Mode and Median are below\n",
      "+-----------+------------------+------------------+\n",
      "|month(DATE)|              Mean|Standard_Deviation|\n",
      "+-----------+------------------+------------------+\n",
      "|          1|15.896774193548387|12.805172721989297|\n",
      "|          2| 13.35862068965517| 13.09180853418292|\n",
      "|          3|14.653225806451612|15.784789500893567|\n",
      "|          4|23.329999999999995| 13.02209725617009|\n",
      "|          5| 36.21935483870967| 8.077246704851957|\n",
      "|          6| 47.42999999999999| 8.877190347997287|\n",
      "|          7| 52.88709677419356|  6.66378723291517|\n",
      "|          8|49.287096774193564| 6.548594740281946|\n",
      "|          9| 41.84499999999999| 5.887660897797832|\n",
      "|         10|31.529032258064507| 9.609052888228815|\n",
      "|         11|29.246666666666666| 8.143448373534971|\n",
      "|         12| 19.95483870967743| 8.854464048157649|\n",
      "+-----------+------------------+------------------+\n",
      "\n",
      "+-----+----+\n",
      "|MONTH|Mode|\n",
      "+-----+----+\n",
      "|    1| 5.7|\n",
      "|    2|15.5|\n",
      "|    3|35.4|\n",
      "|    4|34.1|\n",
      "|    5|37.0|\n",
      "|    6|58.6|\n",
      "|    7|49.3|\n",
      "|    8|44.7|\n",
      "|    9|46.5|\n",
      "|   10|32.2|\n",
      "|   11|36.3|\n",
      "|   12|25.3|\n",
      "+-----+----+\n",
      "\n",
      "+-----+------+\n",
      "|month|Median|\n",
      "+-----+------+\n",
      "|    1|  14.9|\n",
      "|    2|  15.3|\n",
      "|    3|  18.6|\n",
      "|    4|  26.0|\n",
      "|    5|  36.0|\n",
      "|    6|  46.0|\n",
      "|    7|  51.4|\n",
      "|    8|  48.7|\n",
      "|    9|  42.5|\n",
      "|   10|  30.7|\n",
      "|   11|  29.8|\n",
      "|   12|  20.2|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Question5\n",
    "print(\"5.Find the mean, median, mode and standard deviation of the Temperature (column TEMP) for each month for the year 2020. > There should be 12 results for each month\")\n",
    "print(\"The four results Mean, Standard_Deviation, Mode and Median are below\")\n",
    "set_the_df[\"csv_df_2020\"].createOrReplaceTempView(\"temporary_elements\")\n",
    "res = spark.sql(\"select month(DATE), avg(temp) as Mean, STDDEV(temp) as Standard_Deviation from temporary_elements where temp!= 9999.9 group by month(DATE) order by month(DATE)\").show()\n",
    "\n",
    "set_the_df[\"csv_df_2020\"].createOrReplaceTempView(\"temporary_elements\")\n",
    "mode=spark.sql(\"with samp1 as (Select month(date) as MONTH, temp,count(temp) over (partition by temp, month(date)) as cnt from temporary_elements ),  samp2 as (select distinct MONTH, max(cnt) over(partition by MONTH) mx from samp1) select distinct samp2.MONTH , max(samp1.temp) over (partition by samp2.MONTH) as Mode from samp1, samp2 where  samp1.MONTH=samp2.MONTH and samp1.cnt=samp2.mx order by samp2.MONTH\") \n",
    "mode.show()\n",
    "\n",
    "set_the_df[\"csv_df_2020\"].createOrReplaceTempView(\"temporary_elements\")\n",
    "res= spark.sql(\"Select temp,month(date) as month from temporary_elements where temp!= 9999.9 order by month(date) \")\n",
    "res.groupBy(\"month\").agg(func.percentile_approx(\"temp\", 0.5).alias(\"Median\")).orderBy(\"month\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b2c278",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
